{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blue> Recommender System for Images on Social Media </font>\n",
    "**Author:**  Armin Berger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project we seek to create a recommender system that can generate user- specific recommendations. More specifically, collaborative filtering models were used to recommend users of the platform Flicker images that match their preferences. The aim was to provide each user with a top 15 selection of images that are likely to match their preference. To showcase the steps taken in creating and training such recommendation models, this report has been broken down into three sections. \n",
    "\n",
    "1. Firstly, it will be discussed what data was used to train the models. \n",
    "2. Secondly, the two different collaborative filtering models will be introduced and compared. \n",
    "3. Lastly, it will be assessed which model performed better as well as the shortcomings and future improvements of each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Idea of a Recommender System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='rec_1.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install all required packages \n",
    "\n",
    "#!pip install pandas\n",
    "#!pip install matplotlib\n",
    "#!pip install scikit-learn\n",
    "#!pip install nltk\n",
    "#!pip install seaborn\n",
    "#!pip install ggplot\n",
    "#!pip3 install torch\n",
    "#!pip3 install implicit\n",
    "#!pip install Node2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# import all required packages \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math \n",
    "import sklearn as sk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as matplot\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os, sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import random\n",
    "import implicit # important package\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "import scipy.sparse as sparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import scipy as sp\n",
    "import scipy.sparse.linalg as linalg\n",
    "import scipy.cluster.hierarchy as hr\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn.datasets as datasets\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.utils as utils\n",
    "import sklearn.linear_model as linear_model\n",
    "import sklearn.svm as svm\n",
    "import sklearn.cluster as cluster\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize    \n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "import networkx as nx\n",
    "from IPython.display import Image\n",
    "from node2vec import Node2Vec\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "For this project we will provide recommendations to users based on collaborative filltering algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, read in the three data sets provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/arminberger/Desktop/final submission/'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set path of actual current directory and add the location of the data\n",
    "\n",
    "#data_path =  os.getcwd() +'/data/' # path used for earlier training\n",
    "\n",
    "data_path = os.getcwd() + '/' # path of actual current directory\n",
    "\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the three data sets provided and save them\n",
    "\n",
    "# data set to train model\n",
    "train = pd.read_csv(data_path + 'flickr_train_data.csv')\n",
    "\n",
    "# data set to validate model\n",
    "valid = pd.read_csv(data_path + 'flickr_validation_data.csv')\n",
    "\n",
    "# data set to test model\n",
    "test = pd.read_csv(data_path + 'flickr_test_data.csv')\n",
    "\n",
    "# data set that provides additional info on items\n",
    "item_df = pd.read_csv(data_path + 'flickr_item_fea.csv')\n",
    "\n",
    "# data set that provides additional info on user connections\n",
    "links_df = pd.read_csv(data_path + 'flickr_links.csv')\n",
    "\n",
    "# data set that provides additional info on user preference\n",
    "user_df = pd.read_csv(data_path + 'flickr_user_fea.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this we will get a basic overview of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task two groups of data sets were provided to create the desired recommender systems. The first group consists of three CSV files labelled ‘flickr_train_data’, ‘flickr_validation_data’, and ‘flickr_test_data’. Both the train and validation data set have three features ‘user_id’, ‘item_id’, and ‘ratings’, where each unique ‘user_id’ is associated with several ‘item_id’ with which the user has either interacted with or not. The interaction of a user with an item is encoded using a rating of 1, whereas 0 represents no user-item interaction. Using this data, we can train our collaborative filtering models since it enables the model to compare user preferences. Based on user preference similarity, it can recommend images similar users interacted with already. The validation data set then enables us to assess and compare the performance of the different model. Lastly, the test data set is used to provide each user with a top 15 selection of images based upon 100 images presented to a user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating\n",
       "0        0        0       1\n",
       "1        0        1       1\n",
       "2        0        2       1\n",
       "3        0        3       1\n",
       "4        0        4       1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'item_id', 'rating'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valid Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3260</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>390</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>5425</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>8631</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating\n",
       "0        0       20       1\n",
       "1        0     3260       0\n",
       "2        0      390       0\n",
       "3        0     5425       0\n",
       "4        0     8631       0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'item_id', 'rating'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>8906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>8838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>8821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>8756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id\n",
       "0        0     8929\n",
       "1        0     8906\n",
       "2        0     8838\n",
       "3        0     8821\n",
       "4        0     8756"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'item_id'], dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110129, 3)\n",
      "(346600, 3)\n",
      "(346600, 2)\n"
     ]
    }
   ],
   "source": [
    "# get the shape of all three data frame\n",
    "\n",
    "print(train.shape)\n",
    "print(valid.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### train ####\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 110129 entries, 0 to 110128\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count   Dtype\n",
      "---  ------   --------------   -----\n",
      " 0   user_id  110129 non-null  int64\n",
      " 1   item_id  110129 non-null  int64\n",
      " 2   rating   110129 non-null  int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 2.5 MB\n",
      "None\n",
      "\n",
      "#### valid ####\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 346600 entries, 0 to 346599\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count   Dtype\n",
      "---  ------   --------------   -----\n",
      " 0   user_id  346600 non-null  int64\n",
      " 1   item_id  346600 non-null  int64\n",
      " 2   rating   346600 non-null  int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 7.9 MB\n",
      "None\n",
      "\n",
      "#### test ####\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 346600 entries, 0 to 346599\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count   Dtype\n",
      "---  ------   --------------   -----\n",
      " 0   user_id  346600 non-null  int64\n",
      " 1   item_id  346600 non-null  int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 5.3 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# get basic information of all three data frame\n",
    "\n",
    "print('#### train ####')\n",
    "\n",
    "print(train.info())\n",
    "\n",
    "print('')\n",
    "print('#### valid ####')\n",
    "\n",
    "print(valid.info())\n",
    "\n",
    "print('')\n",
    "print('#### test ####')\n",
    "\n",
    "print(test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    110129\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2047    100\n",
       "2387    100\n",
       "2130    100\n",
       "1874    100\n",
       "1618    100\n",
       "       ... \n",
       "1446    100\n",
       "3239    100\n",
       "1190    100\n",
       "2983    100\n",
       "0       100\n",
       "Name: user_id, Length: 3466, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['user_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for the number of users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3466"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(test['user_id'].to_list())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second data set group contained information in another three CSV files labelled ‘flickr_item_fea’, ‘flickr_links’, and ‘flickr_user_fea’. The ‘flickr_item_fea’ contains the pixel information of each image, encoded in 256 columns. The ‘flickr_user_fea’ file contains information on the item a user liked the most. The ‘flickr_links’ provides information on which users are friends with another. Given the underlying assumption that users who are friends on Flicker have similar preferences, one can use this information to provide the recommender system with information on similar users. In this project, the use of both the additional user and image data leads to a substantial recommendation improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Item Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>fea_0</th>\n",
       "      <th>fea_1</th>\n",
       "      <th>fea_2</th>\n",
       "      <th>fea_3</th>\n",
       "      <th>fea_4</th>\n",
       "      <th>fea_5</th>\n",
       "      <th>fea_6</th>\n",
       "      <th>fea_7</th>\n",
       "      <th>fea_8</th>\n",
       "      <th>...</th>\n",
       "      <th>fea_246</th>\n",
       "      <th>fea_247</th>\n",
       "      <th>fea_248</th>\n",
       "      <th>fea_249</th>\n",
       "      <th>fea_250</th>\n",
       "      <th>fea_251</th>\n",
       "      <th>fea_252</th>\n",
       "      <th>fea_253</th>\n",
       "      <th>fea_254</th>\n",
       "      <th>fea_255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.360416</td>\n",
       "      <td>-0.683295</td>\n",
       "      <td>-0.659030</td>\n",
       "      <td>-1.840172</td>\n",
       "      <td>-1.875868</td>\n",
       "      <td>0.369852</td>\n",
       "      <td>-0.806902</td>\n",
       "      <td>-0.210944</td>\n",
       "      <td>-1.371679</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.287771</td>\n",
       "      <td>-0.003345</td>\n",
       "      <td>-1.017979</td>\n",
       "      <td>-0.849153</td>\n",
       "      <td>-1.564582</td>\n",
       "      <td>-1.167882</td>\n",
       "      <td>-1.138140</td>\n",
       "      <td>-0.459417</td>\n",
       "      <td>-1.342706</td>\n",
       "      <td>-0.491899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-5.414928</td>\n",
       "      <td>-4.034106</td>\n",
       "      <td>-6.748904</td>\n",
       "      <td>-4.867840</td>\n",
       "      <td>-8.706087</td>\n",
       "      <td>-8.195160</td>\n",
       "      <td>-5.521785</td>\n",
       "      <td>-5.785634</td>\n",
       "      <td>-6.909437</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.531560</td>\n",
       "      <td>-8.419637</td>\n",
       "      <td>-9.145112</td>\n",
       "      <td>-4.200750</td>\n",
       "      <td>-7.780539</td>\n",
       "      <td>-4.257525</td>\n",
       "      <td>-5.879356</td>\n",
       "      <td>-8.006350</td>\n",
       "      <td>-9.809999</td>\n",
       "      <td>-8.942007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.742383</td>\n",
       "      <td>-0.772285</td>\n",
       "      <td>-0.565367</td>\n",
       "      <td>-1.349759</td>\n",
       "      <td>-0.456425</td>\n",
       "      <td>-0.973115</td>\n",
       "      <td>-1.273366</td>\n",
       "      <td>-0.878384</td>\n",
       "      <td>-0.554383</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.065703</td>\n",
       "      <td>-0.614035</td>\n",
       "      <td>-0.517165</td>\n",
       "      <td>-0.655323</td>\n",
       "      <td>-1.019581</td>\n",
       "      <td>-1.706855</td>\n",
       "      <td>-1.084116</td>\n",
       "      <td>-0.940970</td>\n",
       "      <td>-0.905574</td>\n",
       "      <td>-1.151560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.853681</td>\n",
       "      <td>-1.021450</td>\n",
       "      <td>-0.639012</td>\n",
       "      <td>-2.095687</td>\n",
       "      <td>-1.756671</td>\n",
       "      <td>-0.450270</td>\n",
       "      <td>-0.607497</td>\n",
       "      <td>-0.755120</td>\n",
       "      <td>-0.827463</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.223609</td>\n",
       "      <td>-1.181383</td>\n",
       "      <td>-0.928340</td>\n",
       "      <td>-0.841091</td>\n",
       "      <td>-1.477898</td>\n",
       "      <td>-1.802510</td>\n",
       "      <td>-1.177191</td>\n",
       "      <td>-1.632113</td>\n",
       "      <td>-1.980912</td>\n",
       "      <td>-1.298478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.049173</td>\n",
       "      <td>-0.208819</td>\n",
       "      <td>-1.020380</td>\n",
       "      <td>-1.916308</td>\n",
       "      <td>-1.213041</td>\n",
       "      <td>0.404414</td>\n",
       "      <td>-1.085374</td>\n",
       "      <td>-1.219756</td>\n",
       "      <td>-0.854078</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.427951</td>\n",
       "      <td>-0.035228</td>\n",
       "      <td>-0.856884</td>\n",
       "      <td>-1.142188</td>\n",
       "      <td>-1.742321</td>\n",
       "      <td>-0.767455</td>\n",
       "      <td>0.114727</td>\n",
       "      <td>-0.083121</td>\n",
       "      <td>-1.886625</td>\n",
       "      <td>-0.243228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>8999</td>\n",
       "      <td>-0.985023</td>\n",
       "      <td>-0.817845</td>\n",
       "      <td>0.278331</td>\n",
       "      <td>-2.334727</td>\n",
       "      <td>-0.493657</td>\n",
       "      <td>0.193453</td>\n",
       "      <td>-0.091037</td>\n",
       "      <td>-0.052208</td>\n",
       "      <td>-0.734198</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.820217</td>\n",
       "      <td>-0.208619</td>\n",
       "      <td>-0.181342</td>\n",
       "      <td>-1.032526</td>\n",
       "      <td>-0.452872</td>\n",
       "      <td>-0.667447</td>\n",
       "      <td>-1.118185</td>\n",
       "      <td>0.044350</td>\n",
       "      <td>-1.059369</td>\n",
       "      <td>-1.240149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9000</th>\n",
       "      <td>9000</td>\n",
       "      <td>-0.397968</td>\n",
       "      <td>-0.468231</td>\n",
       "      <td>-1.051147</td>\n",
       "      <td>-1.416116</td>\n",
       "      <td>-1.209115</td>\n",
       "      <td>-0.862491</td>\n",
       "      <td>-1.081148</td>\n",
       "      <td>-1.151810</td>\n",
       "      <td>-1.007816</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.927771</td>\n",
       "      <td>-0.692016</td>\n",
       "      <td>-0.930869</td>\n",
       "      <td>-1.314085</td>\n",
       "      <td>-0.779665</td>\n",
       "      <td>-1.113890</td>\n",
       "      <td>-0.722510</td>\n",
       "      <td>-1.455863</td>\n",
       "      <td>-1.520061</td>\n",
       "      <td>-0.461387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9001</th>\n",
       "      <td>9001</td>\n",
       "      <td>-0.387659</td>\n",
       "      <td>-1.241516</td>\n",
       "      <td>-1.118529</td>\n",
       "      <td>-2.025316</td>\n",
       "      <td>-2.436767</td>\n",
       "      <td>-0.936830</td>\n",
       "      <td>-2.213711</td>\n",
       "      <td>-1.324245</td>\n",
       "      <td>-2.002571</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.409009</td>\n",
       "      <td>-1.352146</td>\n",
       "      <td>-1.068698</td>\n",
       "      <td>-1.768486</td>\n",
       "      <td>-1.604628</td>\n",
       "      <td>-0.842151</td>\n",
       "      <td>-1.341742</td>\n",
       "      <td>-2.386787</td>\n",
       "      <td>-2.120183</td>\n",
       "      <td>-1.107710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9002</th>\n",
       "      <td>9002</td>\n",
       "      <td>-0.973734</td>\n",
       "      <td>-0.804314</td>\n",
       "      <td>-0.733195</td>\n",
       "      <td>-0.608296</td>\n",
       "      <td>-1.129770</td>\n",
       "      <td>-1.349558</td>\n",
       "      <td>-0.599012</td>\n",
       "      <td>-0.946626</td>\n",
       "      <td>-0.947507</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.792138</td>\n",
       "      <td>0.024926</td>\n",
       "      <td>-0.864332</td>\n",
       "      <td>-0.881979</td>\n",
       "      <td>-0.719055</td>\n",
       "      <td>-0.211492</td>\n",
       "      <td>-0.742412</td>\n",
       "      <td>-0.311049</td>\n",
       "      <td>0.139330</td>\n",
       "      <td>-0.453119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9003</th>\n",
       "      <td>9003</td>\n",
       "      <td>-1.662481</td>\n",
       "      <td>-0.705773</td>\n",
       "      <td>-0.121582</td>\n",
       "      <td>-1.788018</td>\n",
       "      <td>-0.482806</td>\n",
       "      <td>-0.196876</td>\n",
       "      <td>-0.221723</td>\n",
       "      <td>-0.649119</td>\n",
       "      <td>0.313073</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.417165</td>\n",
       "      <td>0.572466</td>\n",
       "      <td>-0.451777</td>\n",
       "      <td>-1.373404</td>\n",
       "      <td>-0.463651</td>\n",
       "      <td>-0.905587</td>\n",
       "      <td>0.968037</td>\n",
       "      <td>0.384124</td>\n",
       "      <td>-1.048390</td>\n",
       "      <td>-0.299860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9004 rows × 257 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0     fea_0     fea_1     fea_2     fea_3     fea_4     fea_5  \\\n",
       "0              0 -1.360416 -0.683295 -0.659030 -1.840172 -1.875868  0.369852   \n",
       "1              1 -5.414928 -4.034106 -6.748904 -4.867840 -8.706087 -8.195160   \n",
       "2              2 -0.742383 -0.772285 -0.565367 -1.349759 -0.456425 -0.973115   \n",
       "3              3 -0.853681 -1.021450 -0.639012 -2.095687 -1.756671 -0.450270   \n",
       "4              4 -1.049173 -0.208819 -1.020380 -1.916308 -1.213041  0.404414   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "8999        8999 -0.985023 -0.817845  0.278331 -2.334727 -0.493657  0.193453   \n",
       "9000        9000 -0.397968 -0.468231 -1.051147 -1.416116 -1.209115 -0.862491   \n",
       "9001        9001 -0.387659 -1.241516 -1.118529 -2.025316 -2.436767 -0.936830   \n",
       "9002        9002 -0.973734 -0.804314 -0.733195 -0.608296 -1.129770 -1.349558   \n",
       "9003        9003 -1.662481 -0.705773 -0.121582 -1.788018 -0.482806 -0.196876   \n",
       "\n",
       "         fea_6     fea_7     fea_8  ...   fea_246   fea_247   fea_248  \\\n",
       "0    -0.806902 -0.210944 -1.371679  ... -1.287771 -0.003345 -1.017979   \n",
       "1    -5.521785 -5.785634 -6.909437  ... -6.531560 -8.419637 -9.145112   \n",
       "2    -1.273366 -0.878384 -0.554383  ... -1.065703 -0.614035 -0.517165   \n",
       "3    -0.607497 -0.755120 -0.827463  ... -1.223609 -1.181383 -0.928340   \n",
       "4    -1.085374 -1.219756 -0.854078  ... -1.427951 -0.035228 -0.856884   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "8999 -0.091037 -0.052208 -0.734198  ... -0.820217 -0.208619 -0.181342   \n",
       "9000 -1.081148 -1.151810 -1.007816  ... -0.927771 -0.692016 -0.930869   \n",
       "9001 -2.213711 -1.324245 -2.002571  ... -2.409009 -1.352146 -1.068698   \n",
       "9002 -0.599012 -0.946626 -0.947507  ... -1.792138  0.024926 -0.864332   \n",
       "9003 -0.221723 -0.649119  0.313073  ... -0.417165  0.572466 -0.451777   \n",
       "\n",
       "       fea_249   fea_250   fea_251   fea_252   fea_253   fea_254   fea_255  \n",
       "0    -0.849153 -1.564582 -1.167882 -1.138140 -0.459417 -1.342706 -0.491899  \n",
       "1    -4.200750 -7.780539 -4.257525 -5.879356 -8.006350 -9.809999 -8.942007  \n",
       "2    -0.655323 -1.019581 -1.706855 -1.084116 -0.940970 -0.905574 -1.151560  \n",
       "3    -0.841091 -1.477898 -1.802510 -1.177191 -1.632113 -1.980912 -1.298478  \n",
       "4    -1.142188 -1.742321 -0.767455  0.114727 -0.083121 -1.886625 -0.243228  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "8999 -1.032526 -0.452872 -0.667447 -1.118185  0.044350 -1.059369 -1.240149  \n",
       "9000 -1.314085 -0.779665 -1.113890 -0.722510 -1.455863 -1.520061 -0.461387  \n",
       "9001 -1.768486 -1.604628 -0.842151 -1.341742 -2.386787 -2.120183 -1.107710  \n",
       "9002 -0.881979 -0.719055 -0.211492 -0.742412 -0.311049  0.139330 -0.453119  \n",
       "9003 -1.373404 -0.463651 -0.905587  0.968037  0.384124 -1.048390 -0.299860  \n",
       "\n",
       "[9004 rows x 257 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_df = item_df.rename(columns={'Unnamed: 0': 'item_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>fea_0</th>\n",
       "      <th>fea_1</th>\n",
       "      <th>fea_2</th>\n",
       "      <th>fea_3</th>\n",
       "      <th>fea_4</th>\n",
       "      <th>fea_5</th>\n",
       "      <th>fea_6</th>\n",
       "      <th>fea_7</th>\n",
       "      <th>fea_8</th>\n",
       "      <th>...</th>\n",
       "      <th>fea_246</th>\n",
       "      <th>fea_247</th>\n",
       "      <th>fea_248</th>\n",
       "      <th>fea_249</th>\n",
       "      <th>fea_250</th>\n",
       "      <th>fea_251</th>\n",
       "      <th>fea_252</th>\n",
       "      <th>fea_253</th>\n",
       "      <th>fea_254</th>\n",
       "      <th>fea_255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.360416</td>\n",
       "      <td>-0.683295</td>\n",
       "      <td>-0.659030</td>\n",
       "      <td>-1.840172</td>\n",
       "      <td>-1.875868</td>\n",
       "      <td>0.369852</td>\n",
       "      <td>-0.806902</td>\n",
       "      <td>-0.210944</td>\n",
       "      <td>-1.371679</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.287771</td>\n",
       "      <td>-0.003345</td>\n",
       "      <td>-1.017979</td>\n",
       "      <td>-0.849153</td>\n",
       "      <td>-1.564582</td>\n",
       "      <td>-1.167882</td>\n",
       "      <td>-1.138140</td>\n",
       "      <td>-0.459417</td>\n",
       "      <td>-1.342706</td>\n",
       "      <td>-0.491899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-5.414928</td>\n",
       "      <td>-4.034106</td>\n",
       "      <td>-6.748904</td>\n",
       "      <td>-4.867840</td>\n",
       "      <td>-8.706087</td>\n",
       "      <td>-8.195160</td>\n",
       "      <td>-5.521785</td>\n",
       "      <td>-5.785634</td>\n",
       "      <td>-6.909437</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.531560</td>\n",
       "      <td>-8.419637</td>\n",
       "      <td>-9.145112</td>\n",
       "      <td>-4.200750</td>\n",
       "      <td>-7.780539</td>\n",
       "      <td>-4.257525</td>\n",
       "      <td>-5.879356</td>\n",
       "      <td>-8.006350</td>\n",
       "      <td>-9.809999</td>\n",
       "      <td>-8.942007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.742383</td>\n",
       "      <td>-0.772285</td>\n",
       "      <td>-0.565367</td>\n",
       "      <td>-1.349759</td>\n",
       "      <td>-0.456425</td>\n",
       "      <td>-0.973115</td>\n",
       "      <td>-1.273366</td>\n",
       "      <td>-0.878384</td>\n",
       "      <td>-0.554383</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.065703</td>\n",
       "      <td>-0.614035</td>\n",
       "      <td>-0.517165</td>\n",
       "      <td>-0.655323</td>\n",
       "      <td>-1.019581</td>\n",
       "      <td>-1.706855</td>\n",
       "      <td>-1.084116</td>\n",
       "      <td>-0.940970</td>\n",
       "      <td>-0.905574</td>\n",
       "      <td>-1.151560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.853681</td>\n",
       "      <td>-1.021450</td>\n",
       "      <td>-0.639012</td>\n",
       "      <td>-2.095687</td>\n",
       "      <td>-1.756671</td>\n",
       "      <td>-0.450270</td>\n",
       "      <td>-0.607497</td>\n",
       "      <td>-0.755120</td>\n",
       "      <td>-0.827463</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.223609</td>\n",
       "      <td>-1.181383</td>\n",
       "      <td>-0.928340</td>\n",
       "      <td>-0.841091</td>\n",
       "      <td>-1.477898</td>\n",
       "      <td>-1.802510</td>\n",
       "      <td>-1.177191</td>\n",
       "      <td>-1.632113</td>\n",
       "      <td>-1.980912</td>\n",
       "      <td>-1.298478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.049173</td>\n",
       "      <td>-0.208819</td>\n",
       "      <td>-1.020380</td>\n",
       "      <td>-1.916308</td>\n",
       "      <td>-1.213041</td>\n",
       "      <td>0.404414</td>\n",
       "      <td>-1.085374</td>\n",
       "      <td>-1.219756</td>\n",
       "      <td>-0.854078</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.427951</td>\n",
       "      <td>-0.035228</td>\n",
       "      <td>-0.856884</td>\n",
       "      <td>-1.142188</td>\n",
       "      <td>-1.742321</td>\n",
       "      <td>-0.767455</td>\n",
       "      <td>0.114727</td>\n",
       "      <td>-0.083121</td>\n",
       "      <td>-1.886625</td>\n",
       "      <td>-0.243228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>8999</td>\n",
       "      <td>-0.985023</td>\n",
       "      <td>-0.817845</td>\n",
       "      <td>0.278331</td>\n",
       "      <td>-2.334727</td>\n",
       "      <td>-0.493657</td>\n",
       "      <td>0.193453</td>\n",
       "      <td>-0.091037</td>\n",
       "      <td>-0.052208</td>\n",
       "      <td>-0.734198</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.820217</td>\n",
       "      <td>-0.208619</td>\n",
       "      <td>-0.181342</td>\n",
       "      <td>-1.032526</td>\n",
       "      <td>-0.452872</td>\n",
       "      <td>-0.667447</td>\n",
       "      <td>-1.118185</td>\n",
       "      <td>0.044350</td>\n",
       "      <td>-1.059369</td>\n",
       "      <td>-1.240149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9000</th>\n",
       "      <td>9000</td>\n",
       "      <td>-0.397968</td>\n",
       "      <td>-0.468231</td>\n",
       "      <td>-1.051147</td>\n",
       "      <td>-1.416116</td>\n",
       "      <td>-1.209115</td>\n",
       "      <td>-0.862491</td>\n",
       "      <td>-1.081148</td>\n",
       "      <td>-1.151810</td>\n",
       "      <td>-1.007816</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.927771</td>\n",
       "      <td>-0.692016</td>\n",
       "      <td>-0.930869</td>\n",
       "      <td>-1.314085</td>\n",
       "      <td>-0.779665</td>\n",
       "      <td>-1.113890</td>\n",
       "      <td>-0.722510</td>\n",
       "      <td>-1.455863</td>\n",
       "      <td>-1.520061</td>\n",
       "      <td>-0.461387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9001</th>\n",
       "      <td>9001</td>\n",
       "      <td>-0.387659</td>\n",
       "      <td>-1.241516</td>\n",
       "      <td>-1.118529</td>\n",
       "      <td>-2.025316</td>\n",
       "      <td>-2.436767</td>\n",
       "      <td>-0.936830</td>\n",
       "      <td>-2.213711</td>\n",
       "      <td>-1.324245</td>\n",
       "      <td>-2.002571</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.409009</td>\n",
       "      <td>-1.352146</td>\n",
       "      <td>-1.068698</td>\n",
       "      <td>-1.768486</td>\n",
       "      <td>-1.604628</td>\n",
       "      <td>-0.842151</td>\n",
       "      <td>-1.341742</td>\n",
       "      <td>-2.386787</td>\n",
       "      <td>-2.120183</td>\n",
       "      <td>-1.107710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9002</th>\n",
       "      <td>9002</td>\n",
       "      <td>-0.973734</td>\n",
       "      <td>-0.804314</td>\n",
       "      <td>-0.733195</td>\n",
       "      <td>-0.608296</td>\n",
       "      <td>-1.129770</td>\n",
       "      <td>-1.349558</td>\n",
       "      <td>-0.599012</td>\n",
       "      <td>-0.946626</td>\n",
       "      <td>-0.947507</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.792138</td>\n",
       "      <td>0.024926</td>\n",
       "      <td>-0.864332</td>\n",
       "      <td>-0.881979</td>\n",
       "      <td>-0.719055</td>\n",
       "      <td>-0.211492</td>\n",
       "      <td>-0.742412</td>\n",
       "      <td>-0.311049</td>\n",
       "      <td>0.139330</td>\n",
       "      <td>-0.453119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9003</th>\n",
       "      <td>9003</td>\n",
       "      <td>-1.662481</td>\n",
       "      <td>-0.705773</td>\n",
       "      <td>-0.121582</td>\n",
       "      <td>-1.788018</td>\n",
       "      <td>-0.482806</td>\n",
       "      <td>-0.196876</td>\n",
       "      <td>-0.221723</td>\n",
       "      <td>-0.649119</td>\n",
       "      <td>0.313073</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.417165</td>\n",
       "      <td>0.572466</td>\n",
       "      <td>-0.451777</td>\n",
       "      <td>-1.373404</td>\n",
       "      <td>-0.463651</td>\n",
       "      <td>-0.905587</td>\n",
       "      <td>0.968037</td>\n",
       "      <td>0.384124</td>\n",
       "      <td>-1.048390</td>\n",
       "      <td>-0.299860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9004 rows × 257 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      item_id     fea_0     fea_1     fea_2     fea_3     fea_4     fea_5  \\\n",
       "0           0 -1.360416 -0.683295 -0.659030 -1.840172 -1.875868  0.369852   \n",
       "1           1 -5.414928 -4.034106 -6.748904 -4.867840 -8.706087 -8.195160   \n",
       "2           2 -0.742383 -0.772285 -0.565367 -1.349759 -0.456425 -0.973115   \n",
       "3           3 -0.853681 -1.021450 -0.639012 -2.095687 -1.756671 -0.450270   \n",
       "4           4 -1.049173 -0.208819 -1.020380 -1.916308 -1.213041  0.404414   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "8999     8999 -0.985023 -0.817845  0.278331 -2.334727 -0.493657  0.193453   \n",
       "9000     9000 -0.397968 -0.468231 -1.051147 -1.416116 -1.209115 -0.862491   \n",
       "9001     9001 -0.387659 -1.241516 -1.118529 -2.025316 -2.436767 -0.936830   \n",
       "9002     9002 -0.973734 -0.804314 -0.733195 -0.608296 -1.129770 -1.349558   \n",
       "9003     9003 -1.662481 -0.705773 -0.121582 -1.788018 -0.482806 -0.196876   \n",
       "\n",
       "         fea_6     fea_7     fea_8  ...   fea_246   fea_247   fea_248  \\\n",
       "0    -0.806902 -0.210944 -1.371679  ... -1.287771 -0.003345 -1.017979   \n",
       "1    -5.521785 -5.785634 -6.909437  ... -6.531560 -8.419637 -9.145112   \n",
       "2    -1.273366 -0.878384 -0.554383  ... -1.065703 -0.614035 -0.517165   \n",
       "3    -0.607497 -0.755120 -0.827463  ... -1.223609 -1.181383 -0.928340   \n",
       "4    -1.085374 -1.219756 -0.854078  ... -1.427951 -0.035228 -0.856884   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "8999 -0.091037 -0.052208 -0.734198  ... -0.820217 -0.208619 -0.181342   \n",
       "9000 -1.081148 -1.151810 -1.007816  ... -0.927771 -0.692016 -0.930869   \n",
       "9001 -2.213711 -1.324245 -2.002571  ... -2.409009 -1.352146 -1.068698   \n",
       "9002 -0.599012 -0.946626 -0.947507  ... -1.792138  0.024926 -0.864332   \n",
       "9003 -0.221723 -0.649119  0.313073  ... -0.417165  0.572466 -0.451777   \n",
       "\n",
       "       fea_249   fea_250   fea_251   fea_252   fea_253   fea_254   fea_255  \n",
       "0    -0.849153 -1.564582 -1.167882 -1.138140 -0.459417 -1.342706 -0.491899  \n",
       "1    -4.200750 -7.780539 -4.257525 -5.879356 -8.006350 -9.809999 -8.942007  \n",
       "2    -0.655323 -1.019581 -1.706855 -1.084116 -0.940970 -0.905574 -1.151560  \n",
       "3    -0.841091 -1.477898 -1.802510 -1.177191 -1.632113 -1.980912 -1.298478  \n",
       "4    -1.142188 -1.742321 -0.767455  0.114727 -0.083121 -1.886625 -0.243228  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "8999 -1.032526 -0.452872 -0.667447 -1.118185  0.044350 -1.059369 -1.240149  \n",
       "9000 -1.314085 -0.779665 -1.113890 -0.722510 -1.455863 -1.520061 -0.461387  \n",
       "9001 -1.768486 -1.604628 -0.842151 -1.341742 -2.386787 -2.120183 -1.107710  \n",
       "9002 -0.881979 -0.719055 -0.211492 -0.742412 -0.311049  0.139330 -0.453119  \n",
       "9003 -1.373404 -0.463651 -0.905587  0.968037  0.384124 -1.048390 -0.299860  \n",
       "\n",
       "[9004 rows x 257 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>des</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1431</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>955</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1824</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>592</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63989</th>\n",
       "      <td>3465</td>\n",
       "      <td>2966</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63990</th>\n",
       "      <td>3465</td>\n",
       "      <td>1872</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63991</th>\n",
       "      <td>3465</td>\n",
       "      <td>733</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63992</th>\n",
       "      <td>3465</td>\n",
       "      <td>507</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63993</th>\n",
       "      <td>3465</td>\n",
       "      <td>1817</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63994 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        src   des  weight\n",
       "0         0  1431       1\n",
       "1         0   955       1\n",
       "2         0  1824       1\n",
       "3         0    70       1\n",
       "4         0   592       1\n",
       "...     ...   ...     ...\n",
       "63989  3465  2966       1\n",
       "63990  3465  1872       1\n",
       "63991  3465   733       1\n",
       "63992  3465   507       1\n",
       "63993  3465  1817       1\n",
       "\n",
       "[63994 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>fea_0</th>\n",
       "      <th>fea_1</th>\n",
       "      <th>fea_2</th>\n",
       "      <th>fea_3</th>\n",
       "      <th>fea_4</th>\n",
       "      <th>fea_5</th>\n",
       "      <th>fea_6</th>\n",
       "      <th>fea_7</th>\n",
       "      <th>fea_8</th>\n",
       "      <th>...</th>\n",
       "      <th>fea_246</th>\n",
       "      <th>fea_247</th>\n",
       "      <th>fea_248</th>\n",
       "      <th>fea_249</th>\n",
       "      <th>fea_250</th>\n",
       "      <th>fea_251</th>\n",
       "      <th>fea_252</th>\n",
       "      <th>fea_253</th>\n",
       "      <th>fea_254</th>\n",
       "      <th>fea_255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.238114</td>\n",
       "      <td>-1.020155</td>\n",
       "      <td>-1.370791</td>\n",
       "      <td>-1.892076</td>\n",
       "      <td>-1.615505</td>\n",
       "      <td>-1.056106</td>\n",
       "      <td>-1.260189</td>\n",
       "      <td>-1.537514</td>\n",
       "      <td>-1.279543</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.639152</td>\n",
       "      <td>-1.351084</td>\n",
       "      <td>-1.526210</td>\n",
       "      <td>-1.441551</td>\n",
       "      <td>-1.567562</td>\n",
       "      <td>-1.363613</td>\n",
       "      <td>-1.133677</td>\n",
       "      <td>-1.448653</td>\n",
       "      <td>-1.912987</td>\n",
       "      <td>-1.220462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.142714</td>\n",
       "      <td>-1.012310</td>\n",
       "      <td>-1.109844</td>\n",
       "      <td>-1.806807</td>\n",
       "      <td>-1.533666</td>\n",
       "      <td>-0.894846</td>\n",
       "      <td>-1.360289</td>\n",
       "      <td>-1.159866</td>\n",
       "      <td>-1.184760</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.471417</td>\n",
       "      <td>-1.028792</td>\n",
       "      <td>-1.397979</td>\n",
       "      <td>-1.304754</td>\n",
       "      <td>-1.630009</td>\n",
       "      <td>-1.318048</td>\n",
       "      <td>-1.080598</td>\n",
       "      <td>-1.251735</td>\n",
       "      <td>-1.813930</td>\n",
       "      <td>-0.975978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.698521</td>\n",
       "      <td>-1.605196</td>\n",
       "      <td>-1.328976</td>\n",
       "      <td>-1.492204</td>\n",
       "      <td>-1.541542</td>\n",
       "      <td>-1.155632</td>\n",
       "      <td>-1.297039</td>\n",
       "      <td>-1.485625</td>\n",
       "      <td>-1.348223</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.529016</td>\n",
       "      <td>-1.372415</td>\n",
       "      <td>-1.769800</td>\n",
       "      <td>-1.414428</td>\n",
       "      <td>-1.550462</td>\n",
       "      <td>-1.249593</td>\n",
       "      <td>-1.284915</td>\n",
       "      <td>-1.446422</td>\n",
       "      <td>-1.686583</td>\n",
       "      <td>-1.145916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.328761</td>\n",
       "      <td>-1.267363</td>\n",
       "      <td>-1.243220</td>\n",
       "      <td>-1.721551</td>\n",
       "      <td>-1.182082</td>\n",
       "      <td>-1.320123</td>\n",
       "      <td>-1.115170</td>\n",
       "      <td>-1.122703</td>\n",
       "      <td>-1.140961</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.441997</td>\n",
       "      <td>-0.956042</td>\n",
       "      <td>-1.570591</td>\n",
       "      <td>-1.238835</td>\n",
       "      <td>-1.489867</td>\n",
       "      <td>-1.320204</td>\n",
       "      <td>-1.202990</td>\n",
       "      <td>-1.322884</td>\n",
       "      <td>-1.580290</td>\n",
       "      <td>-0.931394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.132466</td>\n",
       "      <td>-1.026957</td>\n",
       "      <td>-1.019664</td>\n",
       "      <td>-1.649181</td>\n",
       "      <td>-1.382753</td>\n",
       "      <td>-0.776261</td>\n",
       "      <td>-1.221357</td>\n",
       "      <td>-1.061034</td>\n",
       "      <td>-1.041313</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.307160</td>\n",
       "      <td>-0.940088</td>\n",
       "      <td>-1.313026</td>\n",
       "      <td>-1.308773</td>\n",
       "      <td>-1.455208</td>\n",
       "      <td>-1.056440</td>\n",
       "      <td>-0.950118</td>\n",
       "      <td>-1.029165</td>\n",
       "      <td>-1.593675</td>\n",
       "      <td>-0.886145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3461</th>\n",
       "      <td>3461</td>\n",
       "      <td>-0.985914</td>\n",
       "      <td>-0.909812</td>\n",
       "      <td>-0.927938</td>\n",
       "      <td>-1.622281</td>\n",
       "      <td>-0.839910</td>\n",
       "      <td>-0.722871</td>\n",
       "      <td>-0.962304</td>\n",
       "      <td>-0.899670</td>\n",
       "      <td>-0.855909</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.933843</td>\n",
       "      <td>-0.674939</td>\n",
       "      <td>-1.038403</td>\n",
       "      <td>-1.278852</td>\n",
       "      <td>-1.048469</td>\n",
       "      <td>-0.666743</td>\n",
       "      <td>-0.739613</td>\n",
       "      <td>-0.552098</td>\n",
       "      <td>-1.398171</td>\n",
       "      <td>-0.618760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>3462</td>\n",
       "      <td>-1.528833</td>\n",
       "      <td>-1.317308</td>\n",
       "      <td>-1.240643</td>\n",
       "      <td>-1.674515</td>\n",
       "      <td>-1.569974</td>\n",
       "      <td>-1.703787</td>\n",
       "      <td>-1.411700</td>\n",
       "      <td>-1.490773</td>\n",
       "      <td>-1.029417</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.428501</td>\n",
       "      <td>-1.387665</td>\n",
       "      <td>-1.434043</td>\n",
       "      <td>-1.522813</td>\n",
       "      <td>-1.708464</td>\n",
       "      <td>-1.446735</td>\n",
       "      <td>-1.682596</td>\n",
       "      <td>-1.458979</td>\n",
       "      <td>-1.648339</td>\n",
       "      <td>-1.299726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3463</th>\n",
       "      <td>3463</td>\n",
       "      <td>-1.720539</td>\n",
       "      <td>-1.573586</td>\n",
       "      <td>-1.674203</td>\n",
       "      <td>-1.694080</td>\n",
       "      <td>-1.989523</td>\n",
       "      <td>-1.696773</td>\n",
       "      <td>-1.577269</td>\n",
       "      <td>-1.871449</td>\n",
       "      <td>-1.555799</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.949060</td>\n",
       "      <td>-1.380057</td>\n",
       "      <td>-1.958907</td>\n",
       "      <td>-1.681401</td>\n",
       "      <td>-1.988764</td>\n",
       "      <td>-1.625365</td>\n",
       "      <td>-1.604610</td>\n",
       "      <td>-1.557434</td>\n",
       "      <td>-1.909479</td>\n",
       "      <td>-1.597918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3464</th>\n",
       "      <td>3464</td>\n",
       "      <td>-1.138993</td>\n",
       "      <td>-0.935112</td>\n",
       "      <td>-1.003635</td>\n",
       "      <td>-1.717942</td>\n",
       "      <td>-1.402207</td>\n",
       "      <td>-0.945415</td>\n",
       "      <td>-1.292941</td>\n",
       "      <td>-1.198142</td>\n",
       "      <td>-1.123732</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.471624</td>\n",
       "      <td>-1.142418</td>\n",
       "      <td>-1.364630</td>\n",
       "      <td>-1.361708</td>\n",
       "      <td>-1.514338</td>\n",
       "      <td>-1.173796</td>\n",
       "      <td>-0.959845</td>\n",
       "      <td>-1.006382</td>\n",
       "      <td>-1.710620</td>\n",
       "      <td>-1.011344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3465</th>\n",
       "      <td>3465</td>\n",
       "      <td>-1.050942</td>\n",
       "      <td>-1.051671</td>\n",
       "      <td>-1.042454</td>\n",
       "      <td>-1.801846</td>\n",
       "      <td>-1.400316</td>\n",
       "      <td>-0.751316</td>\n",
       "      <td>-1.312807</td>\n",
       "      <td>-1.185146</td>\n",
       "      <td>-1.088689</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.300297</td>\n",
       "      <td>-0.930734</td>\n",
       "      <td>-1.348233</td>\n",
       "      <td>-1.259456</td>\n",
       "      <td>-1.421653</td>\n",
       "      <td>-1.096801</td>\n",
       "      <td>-1.006745</td>\n",
       "      <td>-1.281112</td>\n",
       "      <td>-1.601733</td>\n",
       "      <td>-0.924185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3466 rows × 257 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0     fea_0     fea_1     fea_2     fea_3     fea_4     fea_5  \\\n",
       "0              0 -1.238114 -1.020155 -1.370791 -1.892076 -1.615505 -1.056106   \n",
       "1              1 -1.142714 -1.012310 -1.109844 -1.806807 -1.533666 -0.894846   \n",
       "2              2 -1.698521 -1.605196 -1.328976 -1.492204 -1.541542 -1.155632   \n",
       "3              3 -1.328761 -1.267363 -1.243220 -1.721551 -1.182082 -1.320123   \n",
       "4              4 -1.132466 -1.026957 -1.019664 -1.649181 -1.382753 -0.776261   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "3461        3461 -0.985914 -0.909812 -0.927938 -1.622281 -0.839910 -0.722871   \n",
       "3462        3462 -1.528833 -1.317308 -1.240643 -1.674515 -1.569974 -1.703787   \n",
       "3463        3463 -1.720539 -1.573586 -1.674203 -1.694080 -1.989523 -1.696773   \n",
       "3464        3464 -1.138993 -0.935112 -1.003635 -1.717942 -1.402207 -0.945415   \n",
       "3465        3465 -1.050942 -1.051671 -1.042454 -1.801846 -1.400316 -0.751316   \n",
       "\n",
       "         fea_6     fea_7     fea_8  ...   fea_246   fea_247   fea_248  \\\n",
       "0    -1.260189 -1.537514 -1.279543  ... -1.639152 -1.351084 -1.526210   \n",
       "1    -1.360289 -1.159866 -1.184760  ... -1.471417 -1.028792 -1.397979   \n",
       "2    -1.297039 -1.485625 -1.348223  ... -1.529016 -1.372415 -1.769800   \n",
       "3    -1.115170 -1.122703 -1.140961  ... -1.441997 -0.956042 -1.570591   \n",
       "4    -1.221357 -1.061034 -1.041313  ... -1.307160 -0.940088 -1.313026   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3461 -0.962304 -0.899670 -0.855909  ... -0.933843 -0.674939 -1.038403   \n",
       "3462 -1.411700 -1.490773 -1.029417  ... -1.428501 -1.387665 -1.434043   \n",
       "3463 -1.577269 -1.871449 -1.555799  ... -1.949060 -1.380057 -1.958907   \n",
       "3464 -1.292941 -1.198142 -1.123732  ... -1.471624 -1.142418 -1.364630   \n",
       "3465 -1.312807 -1.185146 -1.088689  ... -1.300297 -0.930734 -1.348233   \n",
       "\n",
       "       fea_249   fea_250   fea_251   fea_252   fea_253   fea_254   fea_255  \n",
       "0    -1.441551 -1.567562 -1.363613 -1.133677 -1.448653 -1.912987 -1.220462  \n",
       "1    -1.304754 -1.630009 -1.318048 -1.080598 -1.251735 -1.813930 -0.975978  \n",
       "2    -1.414428 -1.550462 -1.249593 -1.284915 -1.446422 -1.686583 -1.145916  \n",
       "3    -1.238835 -1.489867 -1.320204 -1.202990 -1.322884 -1.580290 -0.931394  \n",
       "4    -1.308773 -1.455208 -1.056440 -0.950118 -1.029165 -1.593675 -0.886145  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3461 -1.278852 -1.048469 -0.666743 -0.739613 -0.552098 -1.398171 -0.618760  \n",
       "3462 -1.522813 -1.708464 -1.446735 -1.682596 -1.458979 -1.648339 -1.299726  \n",
       "3463 -1.681401 -1.988764 -1.625365 -1.604610 -1.557434 -1.909479 -1.597918  \n",
       "3464 -1.361708 -1.514338 -1.173796 -0.959845 -1.006382 -1.710620 -1.011344  \n",
       "3465 -1.259456 -1.421653 -1.096801 -1.006745 -1.281112 -1.601733 -0.924185  \n",
       "\n",
       "[3466 rows x 257 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = user_df.rename(columns={'Unnamed: 0': 'user_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>fea_0</th>\n",
       "      <th>fea_1</th>\n",
       "      <th>fea_2</th>\n",
       "      <th>fea_3</th>\n",
       "      <th>fea_4</th>\n",
       "      <th>fea_5</th>\n",
       "      <th>fea_6</th>\n",
       "      <th>fea_7</th>\n",
       "      <th>fea_8</th>\n",
       "      <th>...</th>\n",
       "      <th>fea_246</th>\n",
       "      <th>fea_247</th>\n",
       "      <th>fea_248</th>\n",
       "      <th>fea_249</th>\n",
       "      <th>fea_250</th>\n",
       "      <th>fea_251</th>\n",
       "      <th>fea_252</th>\n",
       "      <th>fea_253</th>\n",
       "      <th>fea_254</th>\n",
       "      <th>fea_255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.238114</td>\n",
       "      <td>-1.020155</td>\n",
       "      <td>-1.370791</td>\n",
       "      <td>-1.892076</td>\n",
       "      <td>-1.615505</td>\n",
       "      <td>-1.056106</td>\n",
       "      <td>-1.260189</td>\n",
       "      <td>-1.537514</td>\n",
       "      <td>-1.279543</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.639152</td>\n",
       "      <td>-1.351084</td>\n",
       "      <td>-1.526210</td>\n",
       "      <td>-1.441551</td>\n",
       "      <td>-1.567562</td>\n",
       "      <td>-1.363613</td>\n",
       "      <td>-1.133677</td>\n",
       "      <td>-1.448653</td>\n",
       "      <td>-1.912987</td>\n",
       "      <td>-1.220462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.142714</td>\n",
       "      <td>-1.012310</td>\n",
       "      <td>-1.109844</td>\n",
       "      <td>-1.806807</td>\n",
       "      <td>-1.533666</td>\n",
       "      <td>-0.894846</td>\n",
       "      <td>-1.360289</td>\n",
       "      <td>-1.159866</td>\n",
       "      <td>-1.184760</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.471417</td>\n",
       "      <td>-1.028792</td>\n",
       "      <td>-1.397979</td>\n",
       "      <td>-1.304754</td>\n",
       "      <td>-1.630009</td>\n",
       "      <td>-1.318048</td>\n",
       "      <td>-1.080598</td>\n",
       "      <td>-1.251735</td>\n",
       "      <td>-1.813930</td>\n",
       "      <td>-0.975978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.698521</td>\n",
       "      <td>-1.605196</td>\n",
       "      <td>-1.328976</td>\n",
       "      <td>-1.492204</td>\n",
       "      <td>-1.541542</td>\n",
       "      <td>-1.155632</td>\n",
       "      <td>-1.297039</td>\n",
       "      <td>-1.485625</td>\n",
       "      <td>-1.348223</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.529016</td>\n",
       "      <td>-1.372415</td>\n",
       "      <td>-1.769800</td>\n",
       "      <td>-1.414428</td>\n",
       "      <td>-1.550462</td>\n",
       "      <td>-1.249593</td>\n",
       "      <td>-1.284915</td>\n",
       "      <td>-1.446422</td>\n",
       "      <td>-1.686583</td>\n",
       "      <td>-1.145916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.328761</td>\n",
       "      <td>-1.267363</td>\n",
       "      <td>-1.243220</td>\n",
       "      <td>-1.721551</td>\n",
       "      <td>-1.182082</td>\n",
       "      <td>-1.320123</td>\n",
       "      <td>-1.115170</td>\n",
       "      <td>-1.122703</td>\n",
       "      <td>-1.140961</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.441997</td>\n",
       "      <td>-0.956042</td>\n",
       "      <td>-1.570591</td>\n",
       "      <td>-1.238835</td>\n",
       "      <td>-1.489867</td>\n",
       "      <td>-1.320204</td>\n",
       "      <td>-1.202990</td>\n",
       "      <td>-1.322884</td>\n",
       "      <td>-1.580290</td>\n",
       "      <td>-0.931394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.132466</td>\n",
       "      <td>-1.026957</td>\n",
       "      <td>-1.019664</td>\n",
       "      <td>-1.649181</td>\n",
       "      <td>-1.382753</td>\n",
       "      <td>-0.776261</td>\n",
       "      <td>-1.221357</td>\n",
       "      <td>-1.061034</td>\n",
       "      <td>-1.041313</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.307160</td>\n",
       "      <td>-0.940088</td>\n",
       "      <td>-1.313026</td>\n",
       "      <td>-1.308773</td>\n",
       "      <td>-1.455208</td>\n",
       "      <td>-1.056440</td>\n",
       "      <td>-0.950118</td>\n",
       "      <td>-1.029165</td>\n",
       "      <td>-1.593675</td>\n",
       "      <td>-0.886145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3461</th>\n",
       "      <td>3461</td>\n",
       "      <td>-0.985914</td>\n",
       "      <td>-0.909812</td>\n",
       "      <td>-0.927938</td>\n",
       "      <td>-1.622281</td>\n",
       "      <td>-0.839910</td>\n",
       "      <td>-0.722871</td>\n",
       "      <td>-0.962304</td>\n",
       "      <td>-0.899670</td>\n",
       "      <td>-0.855909</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.933843</td>\n",
       "      <td>-0.674939</td>\n",
       "      <td>-1.038403</td>\n",
       "      <td>-1.278852</td>\n",
       "      <td>-1.048469</td>\n",
       "      <td>-0.666743</td>\n",
       "      <td>-0.739613</td>\n",
       "      <td>-0.552098</td>\n",
       "      <td>-1.398171</td>\n",
       "      <td>-0.618760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>3462</td>\n",
       "      <td>-1.528833</td>\n",
       "      <td>-1.317308</td>\n",
       "      <td>-1.240643</td>\n",
       "      <td>-1.674515</td>\n",
       "      <td>-1.569974</td>\n",
       "      <td>-1.703787</td>\n",
       "      <td>-1.411700</td>\n",
       "      <td>-1.490773</td>\n",
       "      <td>-1.029417</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.428501</td>\n",
       "      <td>-1.387665</td>\n",
       "      <td>-1.434043</td>\n",
       "      <td>-1.522813</td>\n",
       "      <td>-1.708464</td>\n",
       "      <td>-1.446735</td>\n",
       "      <td>-1.682596</td>\n",
       "      <td>-1.458979</td>\n",
       "      <td>-1.648339</td>\n",
       "      <td>-1.299726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3463</th>\n",
       "      <td>3463</td>\n",
       "      <td>-1.720539</td>\n",
       "      <td>-1.573586</td>\n",
       "      <td>-1.674203</td>\n",
       "      <td>-1.694080</td>\n",
       "      <td>-1.989523</td>\n",
       "      <td>-1.696773</td>\n",
       "      <td>-1.577269</td>\n",
       "      <td>-1.871449</td>\n",
       "      <td>-1.555799</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.949060</td>\n",
       "      <td>-1.380057</td>\n",
       "      <td>-1.958907</td>\n",
       "      <td>-1.681401</td>\n",
       "      <td>-1.988764</td>\n",
       "      <td>-1.625365</td>\n",
       "      <td>-1.604610</td>\n",
       "      <td>-1.557434</td>\n",
       "      <td>-1.909479</td>\n",
       "      <td>-1.597918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3464</th>\n",
       "      <td>3464</td>\n",
       "      <td>-1.138993</td>\n",
       "      <td>-0.935112</td>\n",
       "      <td>-1.003635</td>\n",
       "      <td>-1.717942</td>\n",
       "      <td>-1.402207</td>\n",
       "      <td>-0.945415</td>\n",
       "      <td>-1.292941</td>\n",
       "      <td>-1.198142</td>\n",
       "      <td>-1.123732</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.471624</td>\n",
       "      <td>-1.142418</td>\n",
       "      <td>-1.364630</td>\n",
       "      <td>-1.361708</td>\n",
       "      <td>-1.514338</td>\n",
       "      <td>-1.173796</td>\n",
       "      <td>-0.959845</td>\n",
       "      <td>-1.006382</td>\n",
       "      <td>-1.710620</td>\n",
       "      <td>-1.011344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3465</th>\n",
       "      <td>3465</td>\n",
       "      <td>-1.050942</td>\n",
       "      <td>-1.051671</td>\n",
       "      <td>-1.042454</td>\n",
       "      <td>-1.801846</td>\n",
       "      <td>-1.400316</td>\n",
       "      <td>-0.751316</td>\n",
       "      <td>-1.312807</td>\n",
       "      <td>-1.185146</td>\n",
       "      <td>-1.088689</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.300297</td>\n",
       "      <td>-0.930734</td>\n",
       "      <td>-1.348233</td>\n",
       "      <td>-1.259456</td>\n",
       "      <td>-1.421653</td>\n",
       "      <td>-1.096801</td>\n",
       "      <td>-1.006745</td>\n",
       "      <td>-1.281112</td>\n",
       "      <td>-1.601733</td>\n",
       "      <td>-0.924185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3466 rows × 257 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id     fea_0     fea_1     fea_2     fea_3     fea_4     fea_5  \\\n",
       "0           0 -1.238114 -1.020155 -1.370791 -1.892076 -1.615505 -1.056106   \n",
       "1           1 -1.142714 -1.012310 -1.109844 -1.806807 -1.533666 -0.894846   \n",
       "2           2 -1.698521 -1.605196 -1.328976 -1.492204 -1.541542 -1.155632   \n",
       "3           3 -1.328761 -1.267363 -1.243220 -1.721551 -1.182082 -1.320123   \n",
       "4           4 -1.132466 -1.026957 -1.019664 -1.649181 -1.382753 -0.776261   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "3461     3461 -0.985914 -0.909812 -0.927938 -1.622281 -0.839910 -0.722871   \n",
       "3462     3462 -1.528833 -1.317308 -1.240643 -1.674515 -1.569974 -1.703787   \n",
       "3463     3463 -1.720539 -1.573586 -1.674203 -1.694080 -1.989523 -1.696773   \n",
       "3464     3464 -1.138993 -0.935112 -1.003635 -1.717942 -1.402207 -0.945415   \n",
       "3465     3465 -1.050942 -1.051671 -1.042454 -1.801846 -1.400316 -0.751316   \n",
       "\n",
       "         fea_6     fea_7     fea_8  ...   fea_246   fea_247   fea_248  \\\n",
       "0    -1.260189 -1.537514 -1.279543  ... -1.639152 -1.351084 -1.526210   \n",
       "1    -1.360289 -1.159866 -1.184760  ... -1.471417 -1.028792 -1.397979   \n",
       "2    -1.297039 -1.485625 -1.348223  ... -1.529016 -1.372415 -1.769800   \n",
       "3    -1.115170 -1.122703 -1.140961  ... -1.441997 -0.956042 -1.570591   \n",
       "4    -1.221357 -1.061034 -1.041313  ... -1.307160 -0.940088 -1.313026   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3461 -0.962304 -0.899670 -0.855909  ... -0.933843 -0.674939 -1.038403   \n",
       "3462 -1.411700 -1.490773 -1.029417  ... -1.428501 -1.387665 -1.434043   \n",
       "3463 -1.577269 -1.871449 -1.555799  ... -1.949060 -1.380057 -1.958907   \n",
       "3464 -1.292941 -1.198142 -1.123732  ... -1.471624 -1.142418 -1.364630   \n",
       "3465 -1.312807 -1.185146 -1.088689  ... -1.300297 -0.930734 -1.348233   \n",
       "\n",
       "       fea_249   fea_250   fea_251   fea_252   fea_253   fea_254   fea_255  \n",
       "0    -1.441551 -1.567562 -1.363613 -1.133677 -1.448653 -1.912987 -1.220462  \n",
       "1    -1.304754 -1.630009 -1.318048 -1.080598 -1.251735 -1.813930 -0.975978  \n",
       "2    -1.414428 -1.550462 -1.249593 -1.284915 -1.446422 -1.686583 -1.145916  \n",
       "3    -1.238835 -1.489867 -1.320204 -1.202990 -1.322884 -1.580290 -0.931394  \n",
       "4    -1.308773 -1.455208 -1.056440 -0.950118 -1.029165 -1.593675 -0.886145  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3461 -1.278852 -1.048469 -0.666743 -0.739613 -0.552098 -1.398171 -0.618760  \n",
       "3462 -1.522813 -1.708464 -1.446735 -1.682596 -1.458979 -1.648339 -1.299726  \n",
       "3463 -1.681401 -1.988764 -1.625365 -1.604610 -1.557434 -1.909479 -1.597918  \n",
       "3464 -1.361708 -1.514338 -1.173796 -0.959845 -1.006382 -1.710620 -1.011344  \n",
       "3465 -1.259456 -1.421653 -1.096801 -1.006745 -1.281112 -1.601733 -0.924185  \n",
       "\n",
       "[3466 rows x 257 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Create and Adjust Ratings in Test Data\n",
    "\n",
    "The test data set currently doesn't have a rating assoicated with each user / item interaction. Ratings for user / item interactions are viatal however for providing accurate predictions. Thus we have taken three main steps to create and finetune ratings for each user.\n",
    "\n",
    "1. Add a rating column to the test data set and give a default traing of 0\n",
    "2. For each user / item interaction check whether the user has already interacted with an image before (check training set)\n",
    "3. Using the optional user and item data to calculate the cosine similarity of each user / item interaction, then add that cosine similarity to the existing rating\n",
    "\n",
    "\n",
    "We will apply the same set of steps to the ratings of the training data in order train our collaborative filtering model on more 'informative' data.\n",
    "\n",
    "These three steps will enable our collaborative filtering model to produce more accurate predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firtsly, since the test data doesn't have a rating column yet we will add a rating column with the default value of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create rating column, default value 0\n",
    "test['rating'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, in the next step we will combine the train and test data to check if users have already interacted with the item. In the case that useres already interacted with an item, we set the rating to 1. Later on this will enable us to remove these items as potential recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(456729, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create copy of train data set\n",
    "train_new = train\n",
    "\n",
    "# merge the train and test data into one large dataframe\n",
    "train_new = train_new.append(test, ignore_index=True)\n",
    "\n",
    "# sort the dataframe by user_ids\n",
    "train_new = train_new.sort_values(by = 'user_id', ignore_index=True)\n",
    "\n",
    "train_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will isolate all the duplicate entries. A duplicate entry indicates that the user in the test set would have already interated with an item. Thus we will set the rating to 1 in the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1878"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save all duplicate entries in a data\n",
    "duplicate = train_new[train_new.duplicated(['user_id', 'item_id'])]\n",
    "\n",
    "# turn the dataframe into a dict for faster data processing later\n",
    "dup_dict = duplicate.to_dict('split')\n",
    "\n",
    "# create list to save all duplicates in\n",
    "duplicate_list = []\n",
    "\n",
    "# iterate through duplicates in dict dup_dict\n",
    "for i in range(0, len(dup_dict['data'])):\n",
    "    \n",
    "    # append each entry as a tuple to duplicate_list\n",
    "    duplicate_list.append((dup_dict['data'][i][0],dup_dict['data'][i][1]))\n",
    "    \n",
    "# turn list into set to keep only unqiue duplicates \n",
    "duplicate_set = set(duplicate_list)\n",
    "\n",
    "# check numnber of final unique duplicates\n",
    "len(duplicate_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will set the rating of all already seen items to 1 in the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that helps us identify items in the test set users have aleady interacted with\n",
    "# since users would have already interacted with the item it won't be recommended again\n",
    "def set_rate(test_user_id, test_item_id):\n",
    "    \n",
    "    # create a default rating of 0\n",
    "    test_rate = 0\n",
    "    \n",
    "    # create user and item id tuple \n",
    "    data = (test_user_id, test_item_id)\n",
    "    \n",
    "    # check whether user and item id tuple is contained in duplicate set\n",
    "    if data in duplicate_set:\n",
    "        \n",
    "        # if yes, set rating to 1\n",
    "        test_rate = 1\n",
    "    \n",
    "    # return the new rating\n",
    "    return test_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function set_rate() to the whole test dataset\n",
    "test['rating'] = test.apply(lambda row: set_rate(row['user_id'], row['item_id']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>1</td>\n",
       "      <td>8131</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>1</td>\n",
       "      <td>8131</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>2</td>\n",
       "      <td>6950</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>2</td>\n",
       "      <td>6950</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>4</td>\n",
       "      <td>4999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345838</th>\n",
       "      <td>3458</td>\n",
       "      <td>4645</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345921</th>\n",
       "      <td>3459</td>\n",
       "      <td>6969</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345922</th>\n",
       "      <td>3459</td>\n",
       "      <td>6969</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346451</th>\n",
       "      <td>3464</td>\n",
       "      <td>4448</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346452</th>\n",
       "      <td>3464</td>\n",
       "      <td>4448</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3764 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id  item_id  rating\n",
       "110           1     8131       1\n",
       "111           1     8131       1\n",
       "219           2     6950       1\n",
       "220           2     6950       1\n",
       "440           4     4999       1\n",
       "...         ...      ...     ...\n",
       "345838     3458     4645       1\n",
       "345921     3459     6969       1\n",
       "345922     3459     6969       1\n",
       "346451     3464     4448       1\n",
       "346452     3464     4448       1\n",
       "\n",
       "[3764 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many test data entries had their rating set to 1\n",
    "test.loc[test['rating'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all items a user has alredy interated with in the test set has a rating of 1 and will thus be excluded when creating recommendations for useres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thirdly, we will use the image information stored in the optional user and item dataframes to calculate the cosine similarity of each user / item interaction. Then we will add that cosine similarity to the existing rating. This final step will significantly enhance our models ability to produce more accurate predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that helps us calculate the cosine similarity of each user / item interaction\n",
    "# takes user_id, item_id, and rating as inputs and returns the adjusted rating\n",
    "def cal_item_similarity(test_user_id, test_item_id, rating):\n",
    "    \n",
    "    ## Get user data\n",
    "    \n",
    "    # given the test_user_id retreive the user specifc item preference \n",
    "    # this is given in the form of 256 pixel data points each in a seperate column of a dataframe\n",
    "    test_user_id_pref = user_df.loc[user_df['user_id'] == test_user_id]\n",
    "    \n",
    "    # subselect the pixel data out of the dataframe\n",
    "    test_user_id_pref = test_user_id_pref.iloc[0,1:]\n",
    "    \n",
    "    # turn dataframe row into a numpy array\n",
    "    test_user_id_pref = test_user_id_pref.to_numpy()\n",
    "    \n",
    "    # reshape the numpy array to enable calculation of the cosine similarity\n",
    "    test_user_id_pref = test_user_id_pref.reshape(1, -1)\n",
    "\n",
    "    \n",
    "    ## Get item data\n",
    "    \n",
    "    # given the test_item_id retreive the characteristics of each image\n",
    "    # this is given in the form of 256 pixel data points each in a seperate column of a dataframe\n",
    "    test_item_id_pref = item_df.loc[item_df['item_id'] == test_item_id]\n",
    "    \n",
    "    # subselect the pixel data out of the dataframe\n",
    "    test_item_id_pref = test_item_id_pref.iloc[0,1:]\n",
    "    \n",
    "    # turn dataframe row into a numpy array\n",
    "    test_item_id_pref = test_item_id_pref.to_numpy()\n",
    "    \n",
    "    # reshape the numpy array to enable calculation of the cosine similarity\n",
    "    test_item_id_pref = test_item_id_pref.reshape(1, -1)\n",
    "    \n",
    "    # calculate the cosine similarity of each user / item interaction\n",
    "    item_sim = cosine_similarity(test_user_id_pref, test_item_id_pref)\n",
    "    \n",
    "    # add cosine similarity to existing rating\n",
    "    item_sim = float(item_sim) + rating\n",
    "    \n",
    "    # return final rating\n",
    "    return item_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function set_rate() to the whole test dataset\n",
    "test['rating'] = test.apply(lambda row: cal_item_similarity(row['user_id'], row['item_id'], row['rating']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finetuning the ratings we can start creating the top 15 recommendations for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function set_rate() to the whole test dataset\n",
    "train['rating'] = train.apply(lambda row: cal_item_similarity(row['user_id'], row['item_id'], row['rating']),axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Transform Inputs and Train Model\n",
    "\n",
    "In the second step, we will transform the model inputs into sparse matricese. Then we will train the recommender model on that input. For this project we will compare two models.\n",
    "\n",
    "Firstly, the Alternating Least Squares Model using the implicit package. Secondly, the Matrix Factorisation Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1 - Alternating Least Squares (ALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to sparse matrix\n",
    "sparse_content_person = sparse.csr_matrix((train['rating'].astype(float), (train['item_id'], train['user_id'])))\n",
    "sparse_person_content = sparse.csr_matrix((train['rating'].astype(float), (train['user_id'], train['item_id'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model using the implicit library. Since, I was not able to apply traditional cross validation approaches to the implicit library I applied a \"brute force\" method and tried out all values from 1 to 80 for both hyperparatmeters alpha and factors.\n",
    "The best performance was achieved with aplpha = 72 and factors = 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the rate in which we'll increase our confidence in a preference with more interactions\n",
    "alpha = 72\n",
    "\n",
    "# set the data used to train the model\n",
    "data = (sparse_content_person * alpha).astype('double')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Intel MKL BLAS detected. Its highly recommend to set the environment variable 'export MKL_NUM_THREADS=1' to disable its internal multithreading\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea03df3cd784dbebcca287abba84230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# set random seed for the model so the output is reproducable \n",
    "random.seed(10)\n",
    "\n",
    "# set the model parameters\n",
    "model_01 = implicit.als.AlternatingLeastSquares(factors=7, regularization=0.1, iterations=50)\n",
    "\n",
    "# fit the model\n",
    "model_01.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create methode used for making user specific top 15 item recommendations\n",
    "def recommend(user_id, sparse_person_content, person_vecs, content_vecs, num_contents=15):\n",
    "    \n",
    "    # Get the interaction scores from the sparse person content matrix\n",
    "    # idea: for each user we get all item interactions (like or does not like)\n",
    "    person_interactions = sparse_person_content[user_id,:].toarray()\n",
    "    \n",
    "    # Add 1 to everything, so that articles with no interaction yet become equal to 1\n",
    "    person_interactions = person_interactions.reshape(-1) + 1\n",
    "    \n",
    "    # Make articles already interacted with equal to zero\n",
    "    person_interactions[person_interactions > 2] = 0\n",
    "    \n",
    "    # Get dot product of person vector and all content vectors\n",
    "    # this gives me the predcicted rating of all itsm for one user\n",
    "    rec_vector = person_vecs[user_id,:].dot(content_vecs.T).toarray()\n",
    "    \n",
    "    \n",
    "    # Scale this recommendation vector between 0 and 1\n",
    "    min_max = MinMaxScaler()\n",
    "    rec_vector_scaled = min_max.fit_transform(rec_vector.reshape(-1,1))[:,0]\n",
    "    \n",
    "    # Content already interacted have their recommendation multiplied by zero\n",
    "    recommend_vector = person_interactions * rec_vector_scaled\n",
    "    \n",
    "    # Sort the indices of the content into order of best recommendations\n",
    "    item_idx = np.argsort(recommend_vector)[::-1][:num_contents]\n",
    "    \n",
    "    # Start empty list to store item_id and scores\n",
    "    users = []\n",
    "    item_id = []\n",
    "\n",
    "    for idx in item_idx:\n",
    "        \n",
    "        # Append item_id and scores to the list\n",
    "        item_id.append(int(train.item_id.loc[train.item_id == idx].iloc[0]))\n",
    "        \n",
    "        users.append(int(user_id))\n",
    "\n",
    "    recommendations = (users, item_id)\n",
    "    \n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2 - Matrix Factorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the Matrix Factorisation class\n",
    "class MF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_size=100):\n",
    "        super(MF, self).__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
    "        self.user_emb.weight.data.uniform_(0, 0.05)\n",
    "        self.item_emb.weight.data.uniform_(0, 0.05)\n",
    "        \n",
    "    def forward(self, u, v):\n",
    "        u = self.user_emb(u)\n",
    "        v = self.item_emb(v)\n",
    "        return (u*v).sum(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number of user\n",
    "num_users = len(train.user_id.unique())\n",
    "\n",
    "# set the number of items\n",
    "num_items = len(train.item_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters of the Matrix Factorisation model\n",
    "model_02 = MF(num_users, num_items, emb_size=257)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model training function\n",
    "def train_epocs(model, epochs=10, lr=0.01, wd=0.0, unsqueeze=False):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    model.train()\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        users = torch.LongTensor(train.user_id.values) # .cuda()\n",
    "        items = torch.LongTensor(train.item_id.values) #.cuda()\n",
    "        ratings = torch.FloatTensor(train.rating.values) #.cuda()\n",
    "        \n",
    "        if unsqueeze:\n",
    "            ratings = ratings.unsqueeze(1)\n",
    "            \n",
    "        y_hat = model(users, items)\n",
    "        loss = F.mse_loss(y_hat, ratings)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss.item()) \n",
    "    test_loss(model, unsqueeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model testing function\n",
    "def test_loss(model, unsqueeze=False):\n",
    "    model.eval()\n",
    "    \n",
    "    users = torch.LongTensor(valid.user_id.values) #.cuda()\n",
    "    items = torch.LongTensor(valid.item_id.values) #.cuda()\n",
    "    ratings = torch.FloatTensor(valid.rating.values) #.cuda()\n",
    "    \n",
    "    if unsqueeze:\n",
    "        ratings = ratings.unsqueeze(1)\n",
    "        \n",
    "    y_hat = model(users, items)\n",
    "    loss = F.mse_loss(y_hat, ratings)\n",
    "    print(\"test loss %.3f \" % loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model prediction function\n",
    "def test_predict(model, data, unsqueeze=False):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        users = torch.LongTensor(data.user_id.values) # .cuda()\n",
    "        items = torch.LongTensor(data.item_id.values) #.cuda()\n",
    "        ratings = torch.FloatTensor(data.rating.values) #.cuda()\n",
    "        \n",
    "        prediction = model(users, items)\n",
    "        \n",
    "        return prediction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9963433742523193\n",
      "2.487689971923828\n",
      "1.8839313983917236\n",
      "1.2458769083023071\n",
      "0.6527143716812134\n",
      "0.20399388670921326\n",
      "0.008414337411522865\n",
      "0.12350903451442719\n",
      "0.4210858643054962\n",
      "0.6223400235176086\n",
      "test loss 7.074 \n"
     ]
    }
   ],
   "source": [
    "# train the matrix factorisation\n",
    "train_epocs(model_02, epochs=10, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two trained models to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Compare Model Performance using DCG Score on Validation  Data\n",
    "In the following, we will test which of the above two created models performs better at providing accurate top 15 recommendations for any given user. Before that we will provide a broad overview and comparison of the models avialable for recommender systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview Recommender Systems\n",
    "\n",
    "Broadly speaking there are three types of recommender systems. These are Content, Collaborative Filtering, and Latent Factor-based recommender systems. For this task, we deployed a collaborative filtering and a latent factor-based recommender system. This model choice is largely due to the fact that we have access to reliable historic data on previous user- item interactions and that given the availability of such data, these models tend to produce accurate predictions. More specifically the two models used are Matrix Factorisation, a common factor-based model, and Alternating Least Squares (ALS), a common collaborative filtering model. For each model, we will describe the creation process as well as their performance and weakness. Model performance, in this case, will be calculated using the Discounted Cumulative Gain (DCG). This approach discounts recommendations based on how far up the rank a correct recommendation is.\n",
    "\n",
    "#### Matrix Factorisation\n",
    "The Matrix Factorisation model is built on the concept that both users and items can be represented in a low dimensional space. In this space, their properties can be described through their position in the space and thus we can recommend items to users based on their position. This position or location of users and items is expressed using vectors. The basic idea is that the rating that user A has for item B can be determined by calculating the dot product of their locations. Given that dot product, we can assess how much a user will like an item. This type of model attained widespread popularity through the 2006 Netflix Price Recommender System Competition which it won.\n",
    "\n",
    "For this task, we implement a basic Matrix Factorisation model which was trained using the ‘flickr_train_data’. To enhance the accuracy of predictions we added more nuance to the ratings of the data the model was trained and tested on. The current ratings only had 0 and 1 values. Therefore, to add more nuance to these ratings we applied the content recommendation-based strategy of calculating the cosine similarity between users and items. A large cosine similarity will lead to items being prioritized when making user recommendations. Despite these adjustments and the large data set to train the model on, we only achieved a DCG score of 273.82, a substantially worse result than the Alternating Least Squares model.\n",
    "\n",
    "To improve the model’s performance, one could tune the hyperparameters of the model, such as learning rate or training epochs. This was, however, not done due to the large computational complexity in comparison to the marginal performance gain from an underperforming model. Given this poor performance, the model was not chosen for the final user recommendations submitted to Kaggle.\n",
    "\n",
    "#### Alternating Least Squares\n",
    "The Alternating Least Squares model is built on the concept of discovering and using patterns that involve the interactions of multiple actors. The basic idea is that if actor A and actor B have interacted and liked similar items, in this case, images. The model will recommend images to actor A that they haven’t seen but which actor B and similar actors have seen and liked. Reversely, images that were seen and liked by actor A and similar actors will be recommended to actor B. As mentioned earlier providing users with recommendations using this type of model is only possible since we have a large data set of past user-item interactions which enable the identification of such patterns between actors.\n",
    "For this task, we implemented the Alternating Least Squares model using the implicit package. The model built on this package only required the sparse matrix input of the user and item data. Given this input, the model determines patterns between users. To enhance model performance, I tuned the model's hyperparameters alpha and factors. This was done by applying a grid search method, that tried out all values from 1 to 80 for both hyperparameters alpha and factors. The best performance was achieved with alpha = 72 and factors = 7. After training and tuning the model, we created a recommender method that would take a ‘user_id’ and a set of items as input and based on that would return the top 15 recommended items for each ‘user_id’. Since the testing data has no rating column, we have added a rating column with the default value of 0. We then checked if a user had already interacted with an item and set the rating to 1.\n",
    "Like the process used for the Matrix Factorisation model, for the purpose of enhancing the accuracy of these predictions we added more nuance to the ratings of the data the model was trained and tested on. The current ratings only had 0 and 1 values. To add more nuance to these ratings we applied the content recommendation-based strategy of calculating the cosine similarity between users and items. A large cosine similarity will then be indicative of an item matching user preferences. Thus, it will be prioritized when making user recommendations. Moreover, we ensured that duplicate and already interacted with item_ids from the training data set had their ratings set to one, leading them to be ignored in the recommendation process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DCG Performance Model 1 - Alternating Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save new version of the validation data set\n",
    "valid_new = valid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make top 15 recommendations for valid data set\n",
    "\n",
    "# convert the valid data to sparse matrix\n",
    "sparse_person_content_01 = sparse.csr_matrix((valid['rating'].astype(float), (valid['user_id'], valid['item_id'])))\n",
    "\n",
    "# get the trained person vectors, we convert them to csr matrices\n",
    "person_vecs_01 = sparse.csr_matrix(model_01.user_factors)\n",
    "\n",
    "# get the trained content vectors, we convert them to csr matrices\n",
    "content_vecs_01 = sparse.csr_matrix(model_01.item_factors)\n",
    "\n",
    "# create a list of all user_ids\n",
    "valid_users = list(set(valid['user_id']))\n",
    "\n",
    "# create a dataframe to save the top 15 recommendations for each user\n",
    "valid_new_01 = pd.DataFrame({'user_id' : [], 'item_id' : []}, dtype=int)\n",
    "\n",
    "\n",
    "# calculate the recommendations for all users in the data se\n",
    "for user in valid_users:\n",
    "    \n",
    "    # using the recommend() methode create the top 15 recommendations for all users\n",
    "    recommendations = recommend(user, sparse_person_content_01, person_vecs_01, content_vecs_01)\n",
    "    \n",
    "    # for each of the 15 recommendations append them to valid_new_01\n",
    "    for i in range(0, len(recommendations[0])):\n",
    "        \n",
    "        # append each recommendation\n",
    "        valid_new_01 = valid_new_01.append({'user_id' : int(recommendations[0][i]) , 'item_id' : int(recommendations[1][i])}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get DCG score for AIC\n",
    "\n",
    "# set dcg score\n",
    "dcg_score = 0\n",
    "\n",
    "# iterate through each user_id in valid_new_01\n",
    "for i in range(0, max(valid_new_01['user_id'])):\n",
    "    \n",
    "    # for each user id get the items given in the validation dataset\n",
    "    top_item = valid.loc[valid['user_id'] == i]\n",
    "    \n",
    "    # extract the one item that each user liked the most\n",
    "    top_item = top_item.loc[top_item['rating'] == 1]\n",
    "    \n",
    "    # turn the top 15 recommendation dataframe into a list\n",
    "    top_item = top_item[['user_id', 'item_id']].values.tolist()\n",
    "    \n",
    "    # save the top 15 user items as tuple\n",
    "    user_item = (top_item[0][0],top_item[0][1])\n",
    "    \n",
    "    # check the location of the top item that each user liked the most\n",
    "    per_user = valid_new_01.loc[valid_new_01['user_id'] == i]\n",
    "    \n",
    "    # save it in list format\n",
    "    per_user = per_user.values.tolist()\n",
    "    \n",
    "    # iterate through each element in that list\n",
    "    for x in range(0,len(per_user)):\n",
    "        \n",
    "        # when the top item that each user liked the most is found \n",
    "        if (user_item[0], user_item[1]) == (per_user[x][0],per_user[x][1]):\n",
    "            \n",
    "            # calculate the dcg score add it to dcg_score\n",
    "            dcg_score += (1/math.log(2 + x))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3853.046627903283"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check model dcg score\n",
    "dcg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DCG Performance Model 2 - Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save new version of the validation dataset\n",
    "valid_new = valid \n",
    "\n",
    "# add a new column to the validation dataset that saves the prediction made by the Matrix Factorization model\n",
    "valid_new['predictions_mf'] = test_predict(model_02, valid_new)\n",
    "\n",
    "# sort the data frame by user_id and the recommendation score, top score is first\n",
    "valid_new = valid_new.sort_values(['user_id', 'predictions_mf'], ascending=False)\n",
    "\n",
    "# reset the dataframes index\n",
    "valid_new = valid_new.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe to save the top 15 recommendations for each user\n",
    "valid_new_02 = pd.DataFrame({'user_id' : [], 'item_id' : [], 'rating' : []}, dtype=int)\n",
    "\n",
    "# iterate through each user id \n",
    "for i in range(0, max(valid_new['user_id'])):\n",
    "    \n",
    "    # for each user_id retreive all matching item entries in the dataframe valid_new\n",
    "    per_user = valid_new.loc[valid_new['user_id'] == i]\n",
    "    \n",
    "    # only keep the top 15 items\n",
    "    per_user = per_user.iloc[0:15,]\n",
    "    \n",
    "    # set the format of the inforamtion that will be appended\n",
    "    per_user = per_user[['user_id', 'item_id', 'rating']]\n",
    "    \n",
    "    # append the top top 15 items and their ratings per user\n",
    "    valid_new_02 = valid_new_02.append(per_user, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the dcg for MF model\n",
    "\n",
    "# set dcg score\n",
    "dcg_score = 0\n",
    "\n",
    "# iteratate through each user id in dataframe valid_new_02\n",
    "for i in range(0, max(valid_new_02['user_id'])):\n",
    "\n",
    "    # for each user_id extract all entires\n",
    "    per_user = valid_new_02.loc[valid_new_02['user_id'] == i]\n",
    "    \n",
    "    # if non of the 15 recommendations made by the model contain the top item that each user liked the most\n",
    "    if set(per_user['rating']) == {0}:\n",
    "        \n",
    "        # no increase in the dcg score\n",
    "        dcg_score += 0\n",
    "    \n",
    "    # if one of the 15 recommendations made by the model contain the top item that each user liked the most\n",
    "    elif set(per_user['rating']) != {0}:\n",
    "        \n",
    "        # get the top 15 recommendations made by the model\n",
    "        rate = per_user['rating'].reset_index(drop = True)\n",
    "        \n",
    "        # determine the location of the top item that each user liked the most\n",
    "        x = rate.loc[rate == 1]\n",
    "        \n",
    "        # turn that location, a number between 0 and 14 into an integer\n",
    "        x = int(x.index[0])\n",
    "        \n",
    "        # calculat the dcg score of each recommendation for each user and add it to dcg_score\n",
    "        dcg_score += (1/math.log(2 + x))\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267.8394308364356"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check model dcg score\n",
    "dcg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After assessing the performance of both models we observed that the ALS model performed better than the Matrix Factorization Model, based on the DCG Score calculated for both. Thus, we will use the ALS model to provide recommendations based on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 - Recommend Top 15 Images to Users in Test Data\n",
    "In the following, we will return the top 15 recommendations chosen based on the person / content vectors for contents never interacted with for any given user. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Model 1 the flow of the function is as follows:\n",
    "\n",
    "-\tFor each user we extract the interactions score between users and items using the sparse person content matrix\n",
    "-\tSince a higher interaction score leads to the model recommending an item to a user, we add 1 to all interaction scores, thus items which user have not interacted with have a higher likelihood of getting recommended\n",
    "-\tAll items that users already interacted with are set to 0 since we do not want to recommend already seen items to users \n",
    "-\tTo provide the user with recommendations we then take the dot product of person vector and content vectors\n",
    "-\tThis recommendation is then scaled from 0 to 1 to standardize the recommendation amongst items\n",
    "-\tTo give users the best recommendations we sort the items in descending order\n",
    "-\tWe extract only the top 15 recommended items and save their item id in combination with the matching user-id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This recommender method can be applied to both the validation ad testing data. When testing this model approach on the validation data set it retuned a DCG score of 3,891.28, a substantially better result than the Matrix Factorisation model. Given the good performance, this model was chosen for the final user recommendations submitted to Kaggle.\n",
    "\n",
    "Despite the great performance of this Alternating Least Squares model on our data, collaborative filtering models have two main downsides. Firstly, we need a sufficiently large user base to provide relevant recommendations, this is also referred to as the ‘cold start’ problem. Secondly, items that have not been viewed very often are unlikely to be picked up, thus the model tends to favour popular mainstream items that have been viewed by many. Regarding the first issue, we were provided with a large historic data set of user-item interactions which mitigate the cold start problem. The second issue, however, is not mitigated. The data set used contains over 9,000 items which could likely lead to a neglect of less commonly interacted with items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make top 15 recommendations for test data set\n",
    "\n",
    "\n",
    "# convert the test data to sparse matrix\n",
    "sparse_person_content = sparse.csr_matrix((test['rating'].astype(float), (test['user_id'], test['item_id'])))\n",
    "\n",
    "# Get the trained person vectors, convert them to csr matrices\n",
    "person_vecs = sparse.csr_matrix(model_01.user_factors)\n",
    "\n",
    "# Get the trained content vectors, convert them to csr matrices\n",
    "content_vecs = sparse.csr_matrix(model_01.item_factors)\n",
    "\n",
    "# create list of all user_ids in the test data\n",
    "test_users = list(set(test['user_id']))\n",
    "\n",
    "# create a data frame to save the top 15 recommendations per user\n",
    "solution_data = pd.DataFrame({'user_id' : [], 'item_id' : []}, dtype=int)    \n",
    "\n",
    "# calculate the recommendations for all users in the data se\n",
    "for user in test_users:\n",
    "    \n",
    "    # get the top 15 recommendations\n",
    "    recommendations = recommend(user, sparse_person_content, person_vecs, content_vecs)\n",
    "    \n",
    "    # for each recommendation appendit to solution_data\n",
    "    for i in range(0, len(recommendations[0])):\n",
    "    \n",
    "        solution_data = solution_data.append({'user_id' : int(recommendations[0][i]) ,\\\n",
    "                                              'item_id' : int(recommendations[1][i])}, ignore_index=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>7930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>7091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>7568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>7795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>7128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>2607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>8275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>3598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>3450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>6265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>3265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>8552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  item_id\n",
       "0         0     1267\n",
       "1         0     7930\n",
       "2         0     7091\n",
       "3         0     7568\n",
       "4         0     7795\n",
       "5         0     3144\n",
       "6         0     1859\n",
       "7         0     7128\n",
       "8         0     2607\n",
       "9         0     8275\n",
       "10        0     3598\n",
       "11        0     3450\n",
       "12        0     6265\n",
       "13        0     3265\n",
       "14        0     8552"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example: for user_id 5 we have exactly the top 15 recomendations\n",
    "solution_data.loc[solution_data[\"user_id\"] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 - Write Top 15 Images per Users to CSV File\n",
    "\n",
    "We output the final results of the best model in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally we write the edited dataframe to a csv file \n",
    "solution_data.to_csv('26255367_output.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
